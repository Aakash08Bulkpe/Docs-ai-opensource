"use strict";exports.id=876,exports.ids=[876],exports.modules={12869:(e,t,r)=>{r.d(t,{sS:()=>i.sS,m2:()=>s.m,RZ:()=>i.RZ,FS:()=>i.FS,Hh:()=>a.PromptTemplate,BJ:()=>i.BJ});var s=r(30148),i=r(72539);r(32714),s.m;var a=r(33847);r(92600),r(74938),r(23508),i.RZ},76876:(e,t,r)=>{r.d(t,{LLMChain:()=>h});var s=r(26895),i=r(12869),a=r(47673),n=r(43500),l=r(90681);class o extends l.YN{parseResultWithPrompt(e,t,r){return this.parseResult(e,r)}_baseMessageToString(e){return"string"==typeof e.content?e.content:this._baseMessageContentToString(e.content)}_baseMessageContentToString(e){return JSON.stringify(e)}async invoke(e,t){return"string"==typeof e?this._callWithConfig((async(e,t)=>this.parseResult([{text:e}],t?.callbacks)),e,{...t,runType:"parser"}):this._callWithConfig((async(e,t)=>this.parseResult([{message:e,text:this._baseMessageToString(e)}],t?.callbacks)),e,{...t,runType:"parser"})}}class u extends o{parseResult(e,t){return this.parse(e[0].text,t)}async parseWithPrompt(e,t,r){return this.parse(e,r)}_type(){throw new Error("_type not implemented")}}Error,r(54305),r(84387),r(39343),"undefined"!=typeof self&&self.location&&"null"!==self.location.origin&&(self.location.origin,self.location.pathname,location.search);r(68576),r(63177),r(28431),r(64293);class p extends u{constructor(){super(...arguments),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","output_parsers","default"]}),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0})}static lc_name(){return"NoOpOutputParser"}parse(e){return Promise.resolve(e)}getFormatInstructions(){return""}}function c(e){if(function(e){return"function"==typeof e._llmType}(e))return e;if("bound"in e&&a.YN.isRunnable(e.bound))return c(e.bound);if("runnable"in e&&"fallbacks"in e&&a.YN.isRunnable(e.runnable))return c(e.runnable);if("default"in e&&a.YN.isRunnable(e.default))return c(e.default);throw new Error("Unable to extract BaseLanguageModel from llmLike object.")}class h extends n.r{static lc_name(){return"LLMChain"}get inputKeys(){return this.prompt.inputVariables}get outputKeys(){return[this.outputKey]}constructor(e){if(super(e),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0}),Object.defineProperty(this,"prompt",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"llm",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"llmKwargs",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"outputKey",{enumerable:!0,configurable:!0,writable:!0,value:"text"}),Object.defineProperty(this,"outputParser",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.prompt=e.prompt,this.llm=e.llm,this.llmKwargs=e.llmKwargs,this.outputKey=e.outputKey??this.outputKey,this.outputParser=e.outputParser??new p,this.prompt.outputParser){if(e.outputParser)throw new Error("Cannot set both outputParser and prompt.outputParser");this.outputParser=this.prompt.outputParser}}getCallKeys(){return"callKeys"in this.llm?this.llm.callKeys:[]}_selectMemoryInputs(e){const t=super._selectMemoryInputs(e),r=this.getCallKeys();for(const s of r)s in e&&delete t[s];return t}async _getFinalOutput(e,t,r){let s;return s=this.outputParser?await this.outputParser.parseResultWithPrompt(e,t,r?.getChild()):e[0].text,s}call(e,t){return super.call(e,t)}async _call(e,t){const r={...e},s={...this.llmKwargs},i=this.getCallKeys();for(const t of i)t in e&&s&&(s[t]=e[t],delete r[t]);const a=await this.prompt.formatPromptValue(r);if("generatePrompt"in this.llm){const{generations:e}=await this.llm.generatePrompt([a],s,t?.getChild());return{[this.outputKey]:await this._getFinalOutput(e[0],a,t)}}const n=this.outputParser?this.llm.pipe(this.outputParser):this.llm,l=await n.invoke(a,t?.getChild());return{[this.outputKey]:l}}async predict(e,t){return(await this.call(e,t))[this.outputKey]}_chainType(){return"llm"}static async deserialize(e){const{llm:t,prompt:r}=e;if(!t)throw new Error("LLMChain must have llm");if(!r)throw new Error("LLMChain must have prompt");return new h({llm:await s.j_.deserialize(t),prompt:await i.m2.deserialize(r)})}serialize(){const e="serialize"in this.llm?this.llm.serialize():void 0;return{_type:`${this._chainType()}_chain`,llm:e,prompt:this.prompt.serialize()}}_getNumTokens(e){return c(this.llm).getNumTokens(e)}}}};